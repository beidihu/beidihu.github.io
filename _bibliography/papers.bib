---
---

@article{hu2023does,
  title={Does Constructing a Belief Distribution Truly Reduce Overconfidence?},
  author={Hu, Beidi and Simmons, Joseph P},
  journal={Journal of Experimental Psychology: General},
  volume={152},
  number={2},
  pages={571},
  publisher={American Psychological Association},
  doi={https://doi.org/10.1037/xge0001291},
  pdf={https://drive.google.com/file/d/1VQeJd3eVyDd2tWwKvoVtk_wrmSZqFO6C/view?usp=sharing},
  ssrn={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4180883},
  supp={https://drive.google.com/file/d/1V68oK10_pcPcvWKorgNAS-GfnTqOHLO1/view?usp=sharing},
  researchbox={https://researchbox.org/314},
  abstract={Can overconfidence be reduced by asking people to provide a belief distribution over all possible outcomes—that is, by asking them to indicate how likely all possible outcomes are? Although prior research suggests that the answer is “yes,” that research suffers from methodological confounds that muddle its interpretation. In our research, we remove these confounds to investigate whether providing a belief distribution truly reduces overconfidence. In 10 studies, participants made predictions about upcoming sports games or other participants’ preferences, and then indicated their confidence in these predictions using rating scales, likelihood judgments, and/or incentivized wagers. Contrary to prior research, and to our own expectations, we find that providing a belief distribution usually increases overconfidence, because doing so seems to reinforce people’s prior beliefs.},
  summary={Past work says yes. Our paper finds the opposite.},
  selected={True}
}

@article{hu2025different,
  title={Different Methods Elicit Different Belief Distributions.},
  author={Hu, Beidi and Simmons, Joseph P},
  journal={Journal of Experimental Psychology: General},
  volume={154},
  number={2},
  pages={476},
  year={2025},
  publisher={American Psychological Association},
  doi={https://doi.org/10.1037/xge0001655},
  pdf={https://drive.google.com/file/d/1ICWKU2mY7NsI6m6bhYRP2AUKXPzTm7J6/view?usp=sharing},
  ssrn={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4926793},
  supp={https://drive.google.com/file/d/1W6q1PN-IjHcYKfeH4igSL8iUo3vOlbZv/view?usp=sharing},
  researchbox={https://researchbox.org/569},
  abstract={When eliciting people’s forecasts or beliefs, you can ask for a point estimate—for example, what is the most likely state of the world?—or you can ask for an entire distribution of beliefs—for example, how likely is every possible state of the world? Eliciting belief distributions potentially yields more information, and researchers have increasingly tried to do so. In this article, we show that different elicitation methods elicit different belief distributions. We compare two popular methods used to elicit belief distributions: Distribution Builder and Sliders. In 10 preregistered studies (N = 14,553), we find that Distribution Builder elicits more accurate belief distributions than Sliders, except when true distributions are right-skewed, for which the results are mixed. This result holds when we assess accuracy (a) relative to a normative benchmark and (b) relative to participants’ own beliefs. Our evidence suggests that participants approach these two methods differently: Sliders users are more likely to start with the lowest bins in the interface, which in turn leads them to put excessive mass in those bins. Our research sheds light on the process by which people construct belief distributions while offering a practical recommendation for future research: All else equal, Distribution Builder yields more accurate belief distributions.},
  summary={Belief distribution elicitations are on the rise. We find that two functionally equivalent methods elicit different belief distributions.}
}

@article{hu2025should,
  title={How Should Time Estimates Be Structured to Increase Customer Satisfaction?},
  author={Hu, Beidi and Gaertig, Celia and Dietvorst, Berkeley J},
  journal={Management Science},
  volume={71},
  number={9},
  pages={7497},
  year={2025},
  publisher={INFORMS},
  doi={https://doi.org/10.1287/mnsc.2023.00137},
  pdf={https://drive.google.com/file/d/1NutuW7y_iAOzTc7ZSVKkAJtM1I5NOI57/view?usp=sharing},
  ssrn={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4927151},
  supp={https://drive.google.com/file/d/1auz8ySRF948fTKYOlxUJXZ9DKX5arl7i/view?usp=sharing},
  researchbox={https://researchbox.org/482},
  abstract={Businesses across industries, such as food delivery apps and GPS navigation systems, routinely provide customers with time estimates in inherently uncertain contexts. How does the format of these time estimates affect customers’ satisfaction? In particular, should companies provide customers with a point estimate representing the best estimate, or should they communicate the inherent uncertainty in outcomes by providing a range estimate? In eight preregistered experiments (N = 5,323), participants observed time estimates provided by an app, and we manipulated whether the app presented the time estimates as a point estimate (e.g., “Your food will arrive in 45 minutes.”) or a range (e.g., “Your food will arrive in 40–50 minutes.”). After participants learned about the app’s prediction performance by sampling a set of past outcomes, we measured participants’ evaluation of the app. We find that participants judged the app more positively when it provided a range rather than a point estimate. These results held across different domains, different time durations, different underlying outcome distributions, and an incentive-compatible design. We also find that this preference is not simply due to people’s dislike of late outcomes, as participants also rated ranges more positively than conservative point estimates corresponding to the upper (i.e., later) bound of the range. These findings suggest that companies can increase customer satisfaction with realized time estimates by communicating the uncertainty inherent in these time estimates.},
  summary={Customers judge a digital platform (e.g., food delivery app, GPS app) more positively when it provides time estimates as ranges rather than point estimates.}
}

@article{hu2026choice,
  title={Choice Set Size Neglect in Predicting Others' Preferences.},
  author={Hu, Beidi and Moon, Alice and VanEpps, Eric},
  journal={Psychological Science},
  year={2026},
  publisher={APA},
  doi={https://doi.org/10.1177/09567976251400333},
  pdf={https://drive.google.com/file/d/1XdQY3ojmJGa25bNS_8MxItnJ8SSvqbAU/view?usp=sharing},
  ssrn={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5708322},
  supp={https://drive.google.com/file/d/1BQg2tnA7vGHSK9ipjPvPK1q5ZiENnJYh/view?usp=sharing},
  researchbox={https://researchbox.org/755},
  abstract={An inherent feature of any choice is the set size from which that choice is made—i.e., the number of available options in a choice set. Choice set size impacts the likelihood of landing on a more-preferred option: Larger sets are more likely to contain an option matching one’s preferences. Despite this, six preregistered experiments (N = 10,092 U.S. adults) demonstrate that people consistently underestimate the effect of set size when predicting others’ liking for a chosen option. We propose this effect arises because, although people recognize that set size predicts liking of a chosen option, they typically fail to attend to it when considering others’ choices. As such, this effect attenuates when attention is drawn to set size, specifically: (a) when considering multiple set sizes simultaneously, (b) when the decision process is framed as ranking rather than choosing, or (c) when prompted to recall set size before predicting others’ preferences.},
  summary={Although every choice comes with a set size, people consistently underestimate how it influences others’ preferences when observing their choices.}
}

